<!--
# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->

# FasterTransformer BERT Triton Backend

The FasterTransformer BERT implementation are in [bert_guide.md](https://github.com/NVIDIA/FasterTransformer/blob/main/docs/bert_guide.md).

## Table Of Contents
 
- [FasterTransformer BERT Triton Backend](#fastertransformer-bert-triton-backend)
  - [Table Of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Setup Environment](#setup-environment)
    - [How to set the model configuration](#how-to-set-the-model-configuration)
    - [Prepare Triton Bert model store](#prepare-triton-bert-model-store)
  - [Run Serving on Single Node](#run-serving-on-single-node)
    - [Run serving directly](#run-serving-directly)
  - [Run Triton server on multiple nodes](#run-triton-server-on-multiple-nodes)
    - [Prepare Triton model store for multi-node setup](#prepare-triton-model-store-for-multi-node-setup)
    - [Run on cluster with Enroot/Pyxis support](#run-on-cluster-with-enrootpyxis-support)

## Introduction

This document describes how to serve the `BERT` model by FasterTransformer Triton backend. This backend is only an interface to call FasterTransformer in Triton. All implementation are in [FasterTransformer repo](https://github.com/NVIDIA/FasterTransformer). Note that the Bert backend only handle the transformer layers. It does not contains the embedding lookup, layernorm after embedding lookup and the dense layer after transformer layers.

## Setup Environment

Follow the guide in [`README.md`](../README.md) to setup the environment and prepare docker image. We assume users already build the docker here.

### How to set the model configuration

In BERT triton backend, the serving configuration is controlled by `config.pbtxt`. We provide an example in `all_models/bert/fastertransformer/config.pbtxt`. It contains the input parameters, output parameters, some other settings like `tensor_para_size` and `model_checkpoint_path`. 

We use the `config.ini` in the `model_checkpoint_path` to control the model hyper-parameters like head number, head size and transformer layers. We also provide an example in `all_models/bert/fastertransformer/1/config.ini` which assume we set the `model_checkpoint_path` as `all_models/bert/fastertransformer/1/`. The `config.ini` will generated by checkpoint converter when user convert model. User can also change the setting to run tests on custom model size to benchmark.  

The following table shows the details of these settings:

* Settings in config.pbtxt

| Classification |            Name            |         Tensor/Parameter Shape          |   Data Type    |                                      Description                                      |
| :------------: | :------------------------: | :-------------------------------------: | :------------: | :-----------------------------------------------------------------------------------: |
|     input      |                            |                                         |                |                                                                                       |
|                |    `input_hidden_state`    | [batch_size, seq_len, hidden_dimension] | FP32/FP16/BF16 |           input hidden state after embedding lookup and layer normalization           |
|                |     `sequence_length`      |              [batch_size]               |     int32      |                          real sequence length of each input                           |
|     output     |                            |                                         |                |                                                                                       |
|                |   `output_hidden_state`    | [batch_size, seq_len, hiddem_dimension] | FP32/FP16/BF16 | output hidden state after transformer layers. Data type of output must same to input. |
|   parameter    |                            |                                         |                |                                                                                       |
|                |     `tensor_para_size`     |                                         |      int       |                        parallelism ways in tensor parallelism                         |
|                |    `pipeline_para_size`    |                                         |      int       |                       parallelism ways in pipeline parallelism                        |
|                |        `data_type`         |                                         |     string     |          infernce data type: fp32 = float32, fp16 = float16, bf16 = bfloat16          |
|                | `enable_custom_all_reduce` |                                         |      bool      |                            use custom all reduction or not                            |
|                |        `model_type`        |                                         |     string     |                                    must use `bert`                                    |
|                |  `model_checkpoint_path`   |                                         |     string     |                  the path to save `config.ini` and weights of model                   |
|                |        `int8_mode`         |                                         |      int       |                   mode for int8 inference. Still not supported yet.                   |
|                |        `is_sparse`         |                                         |      bool      |           Is using Ampere sparsity for inference. Still not supported yet.            |
|                |    `is_remove_padding`     |                                         |      bool      |                           Is remove the padding of inputs.                            |

* Note that the data type of `input_hidden_state` and `output_hidden_state` are determined by `data_type` of parameter automatically. 

### Prepare Triton Bert model store

Download Bert model checkpoint:

```shell
docker run -it --rm --gpus=all --shm-size=1g --ulimit memlock=-1 -v ${WORKSPACE}:${WORKSPACE} -w ${WORKSPACE} ${TRITON_DOCKER_IMAGE} bash
# now in docker
export WORKSPACE=$(pwd)

sudo apt-get install git-lfs
git lfs install
git lfs clone https://huggingface.co/bert-base-uncased # Download model from huggingface
git clone https://github.com/NVIDIA/FasterTransformer.git # To convert checkpoint
export PYTHONPATH=${WORKSPACE}/FasterTransformer:${PYTHONPATH}
python3 FasterTransformer/examples/pytorch/bert/utils/huggingface_bert_convert.py \
        -in_file bert-base-uncased/ \
        -saved_dir ${WORKSPACE}/all_models/bert/fastertransformer/1/ \
        -infer_tensor_para_size 2
```

We need to convert to format handled by FasterTransformer. 
If you want to run the model with tensor parallel size 4 and pipeline parallel size 2,
you should convert checkpoints with `-infer_tensor_para_size = [tensor_para_size], i.e. -infer_tensor_para_size = 4`. (Users don't need to care the pipeline parallel size during converting model)
We will convert it directly to directory structure which later we'd use as Triton model store.

Then we will get the model weights (`xxx.bin`) and the config file of model (`config.ini`) in the `${WORKSPACE}/all_models/bert/fastertransformer/1/2-gpu/`. The `config.ini` file contains the hyper-parameters of both encoder and decoder. Note that user need to change the `model_checkpoint_path` to `../all_models/bert/fastertransformer/1/1-gpu/`.

## Run Serving on Single Node

### Run serving directly

Before launching server, we suggest run the gemm test first like what we mention [here](https://github.com/NVIDIA/FasterTransformer/blob/main/docs/bert_guide.md#run-fastertransformer-bert-on-c). The gemm test program is put at `/workspace/build/fastertransformer_backend/build/bin/bert_gemm`.

```bash
sudo pip3 install torch==1.12.1+cu116 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
/workspace/build/fastertransformer_backend/build/bin/bert_gemm 32 32 12 64 1 0 2
CUDA_VISIBLE_DEVICES=0,1 mpirun -n 1 --allow-run-as-root /opt/tritonserver/bin/tritonserver  --model-repository=${WORKSPACE}/all_models/bert/ &
python3 ${WORKSPACE}/tools/bert/identity_test.py \
        --hf_ckpt_path ./bert-base-uncased/ \
        --num_runs 100 \
        --inference_data_type fp16
```

The results would be like

```bash
abs max diff: 0.08879026770591736
abs mean diff: 0.0029141176491975784
[INFO] execution time: 2.181591 ms
```

* Note: If user encounter `[ERROR] world_size (4) should equal to tensor_para_size_ * pipeline_para_size_ (1 * 1 here)`, please check that the GPU number of your device and set the GPUs you want to use by `CUDA_VISIBLE_DEVICES`. 
* Recommend modifying the SERVER_TIMEOUT of common/util.sh to longer time
* Note that we use random inputs and generate random outputs in `tools/bert/identity_test.py`. 
* Note that `inference_data_type` of `identity_test.py` must same to `data_type` of `config.pbtxt`.

## Run Triton server on multiple nodes

### Prepare Triton model store for multi-node setup

For this experiment you need to [prepare Triton bert model store](#prepare-triton-bert-model-store):
- properly convert the checkpoint to FasterTransformer format
- update Triton model configuration

We do suggest:
- `tensor_para_size` = number of gpus in one node (e.g. 8 for DGX A100)
- `layer_para_size` = number of nodes

Other Triton model configuration parameters should be updated as for single node setup.

Model store should be placed on network file system available for all cluster nodes on which Triton will run.

### Run on cluster with Enroot/Pyxis support

First allocate two nodes:

```bash
salloc -A account_name -t 10:00:00 -N 2
```

Then run the script shown below to start two nodes' server.
-N and -n should be equal to the number of nodes because we start one process per node. If you need to run on two nodes, then -N 2 and -n 2.
Remember to change `tensor_para_size` and `pipeline_para_size` as suggested in [MPI Launching with Tensor Parallel size/ Pipeline Parallel Size Setting](../README.md#mpi-launching-with-tensor-parallel-size-and-pipeline-parallel-size-setting) if you run on multiple nodes. 

```bash
WORKSPACE="/workspace" # the dir you build the docker
IMAGE="github_or_gitlab/fastertransformer/multi-node-ft-triton-backend:latest"
CMD="/opt/tritonserver/bin/tritonserver --model-repository=$WORKSPACE/fastertransformer_backend/all_models/bert"
srun -N 2 -n 2 --mpi=pmix -o inference_server.log \
               --container-mounts /home/account/your_network_shared_space/triton:/workspace \
               --container-name multi-node-ft-triton \
               --container-image $IMAGE \
               bash -c "$CMD"
```

Then, you need to run the server on the background since it will not detach by itself. You can enter and commands `ctrl D` and `bg` or run the script above with `sbatch`.

Next, enter the master triton node (the node where MPI_Rank = 0, normally it is the allocated node with the smallest id) when servers have been started shown in the inference log:

```bash
srun -w master-node-name --overlap \
                         --container-name multi-node-ft-triton \
                         --container-mounts /home/account/your_network_shared_space/triton:/workspace \
                         --pty bash # --overlap may not be needed in your slurm environment
```

Finally, run the client in the master triton node:

```bash
python3 fastertransformer_backend/tools/bert/identity_test.py --hf_ckpt_path ./bert-base-uncased/ --num_runs 100
```

You can refer to `inference_server.log` on the login-node for the inference server log.

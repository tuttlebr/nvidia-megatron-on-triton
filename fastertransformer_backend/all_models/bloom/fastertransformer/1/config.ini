[gpt]
model_name=bloom-560m
num_layer=24
head_num=16
inter_size=4096
size_per_head=64
vocab_size=250880
tensor_para_size=1
weight_data_type=fp32
model_variant=bloom-pre
layernorm_eps=1e-05
layernorm_type=pre_layernorm
activation_type=Gelu
has_positional_encoding=False
has_pre_decoder_layernorm=True
has_post_decoder_layernorm=True
use_attention_linear_bias=True
start_id=1
end_id=2

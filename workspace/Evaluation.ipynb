{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a55710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "from tritonclient.utils import np_to_triton_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec39726",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"localhost:8000\"\n",
    "MODEL = \"ensemble\"\n",
    "IS_RETURN_LOG_PROBS = True\n",
    "START_ID = 220\n",
    "END_ID = 50256\n",
    "RANDOM_SEED = 1\n",
    "client = httpclient.InferenceServerClient(URL, concurrency=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9bb6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensor(name, input):\n",
    "    tensor = httpclient.InferInput(\n",
    "        name, input.shape, np_to_triton_dtype(input.dtype))\n",
    "    tensor.set_data_from_numpy(input)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840279db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(data):\n",
    "    bad_words_list = np.array([data[\"bad_words_list\"]], dtype=object)\n",
    "    stop_words_list = np.array([data[\"stop_words_list\"]], dtype=object)\n",
    "    input0_data = np.array(data[\"prompt\"]).astype(object)\n",
    "    output0_len = np.ones_like(data[\"prompt\"]).astype(np.uint32) * data[\"tokens_to_generate\"]\n",
    "    runtime_top_k = (data[\"runtime_top_k\"] * np.ones([input0_data.shape[0], 1])).astype(np.uint32)\n",
    "    runtime_top_p = data[\"runtime_top_p\"] * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    beam_search_diversity_rate = data[\"beam_search_diversity_rate\"] * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    temperature = data[\"temperature\"] * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    len_penalty = data[\"len_penalty\"] * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    repetition_penalty = data[\"repetition_penalty\"] * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    random_seed = data[\"random_seed\"] * np.ones([input0_data.shape[0], 1]).astype(np.uint64)\n",
    "    is_return_log_probs = data[\"is_return_log_probs\"] * np.ones([input0_data.shape[0], 1]).astype(bool)\n",
    "    beam_width = (data[\"beam_width\"] * np.ones([input0_data.shape[0], 1])).astype(np.uint32)\n",
    "    start_id = data[\"start_id\"] * np.ones([input0_data.shape[0], 1]).astype(np.uint32)\n",
    "    end_id = data[\"end_id\"] * np.ones([input0_data.shape[0], 1]).astype(np.uint32)\n",
    "\n",
    "    inputs = [\n",
    "        prepare_tensor(\"INPUT_0\", input0_data),\n",
    "        prepare_tensor(\"INPUT_1\", output0_len),\n",
    "        prepare_tensor(\"INPUT_2\", bad_words_list),\n",
    "        prepare_tensor(\"INPUT_3\", stop_words_list),\n",
    "        prepare_tensor(\"runtime_top_k\", runtime_top_k),\n",
    "        prepare_tensor(\"runtime_top_p\", runtime_top_p),\n",
    "        prepare_tensor(\"beam_search_diversity_rate\", beam_search_diversity_rate),\n",
    "        prepare_tensor(\"temperature\", temperature),\n",
    "        prepare_tensor(\"len_penalty\", len_penalty),\n",
    "        prepare_tensor(\"repetition_penalty\", repetition_penalty),\n",
    "        prepare_tensor(\"random_seed\", random_seed),\n",
    "        prepare_tensor(\"is_return_log_probs\", is_return_log_probs),\n",
    "        prepare_tensor(\"beam_width\", beam_width),\n",
    "        prepare_tensor(\"start_id\", start_id),\n",
    "        prepare_tensor(\"end_id\", end_id),\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e123221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_outputs(result):\n",
    "    completions = result.as_numpy(\"OUTPUT_0\")\n",
    "    formatted_completions = []\n",
    "    for completion in completions:\n",
    "        tmp_string = completion.decode(\"utf-8\")\n",
    "        tmp_string = re.sub('<\\|endoftext\\|>', \"\", tmp_string)\n",
    "        formatted_completions.append(tmp_string)\n",
    "        \n",
    "    return formatted_completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3197bf",
   "metadata": {},
   "source": [
    "## Sample Prompts\n",
    "\n",
    "### Tuning Parameters\n",
    "| **Parameter** | **Description** |\n",
    "|---|---|\n",
    "| Number of Tokens | _Specifies how much text to generate. Tokens can be either an entire word, or parts of words. For English, 100 tokens form approximately 75 words._ |\n",
    "| Temperature | _Controls the randomness of selecting the next token during text generation. Lower values reduce randomness, suitable for tasks with a correct answer such as question answering or summarization. Higher values increase randomness, suitable for tasks that require creativity. The [0.5, 0.8] range is a good starting point for experimentation._ |\n",
    "| Top K | _Controls the randomness of selecting the next token during text generation. The number of highest-probability tokens to keep, from which the next token will be selected at random. Lower values reduce randomness, suitable for tasks with a correct answer such as question answering or summarization. Higher values increase randomness, suitable for tasks that require creativity. 0 means Top K is not used. 1 means greedy decoding, that is, always selecting the most probable token next._ |\n",
    "| Top P | _Controls the randomness of selecting the next token during text generation. This determines the minimum number of highest-probability tokens whose probabilities sum to or exceed the Top P value, from which the next token will be selected at random. Lower values reduce randomness, suitable for tasks with a correct answer such as question answering or summarization. Higher values increase randomness, suitable for tasks that require creativity._ |\n",
    "| Repetition Penalty | _How much to penalize tokens based on how frequently they occur in the text. A value of 1 means no penalty, while values larger than 1 discourage repeated tokens._ |\n",
    "| Length Penalty | _Only applies to beam search, that is, when the beam width is >1. Larger values penalize long candidates more heavily thus preferring shorter candidates._ |\n",
    "| Beam Search Diversity Rate | _Only applies to beam search, that is, when the beam width is >1. A higher value encourages beam search to return a more diverse set of candidates._ |\n",
    "| Beam Width | _The number of concurrent candidates to keep track of during beam search. Higher values increase the chance of finding a good output but also require more computation. Streaming is supported with a “beam width” hyperparameter set to 1 only._ |\n",
    "| Random Seed | _The model generates random results. Changing the random seed alone will produce a different response with similar characteristics. It is possible to reproduce results by fixing the random seed (assuming all other hyperparameters are also fixed)._ |\n",
    "| Stop Words | _Set of character sequences, upon generating any of which, the API will stop generating any further text prematurely, even if the output length has not yet reached the specified number of tokens. It is useful to design a stopping template in the examples given to the model so that it can learn to stop appropriately upon completing an intended task._ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cd65f",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8361fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following article:\n",
      "Article: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "Summary: The Transformer architecture based solely on the attention mechanism deliver superior quality on several translation tasks while being more parallelizable and requiring significantly less time to train compared to recurrence and convolution alternatives.\n",
      "\n",
      "===\n",
      "\n",
      "Summarize the following article:\n",
      "Article: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\n",
      "\n",
      "Summary: We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.\n",
      "\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "summarization = {\n",
    "    \"prompt\": [[\"Summarize the following article:\\nArticle: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\nSummary: The Transformer architecture based solely on the attention mechanism deliver superior quality on several translation tasks while being more parallelizable and requiring significantly less time to train compared to recurrence and convolution alternatives.\\n\\n===\\n\\nSummarize the following article:\\nArticle: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\\n\\nSummary:\"]],\n",
    "    \"stop_words_list\": [\"===\",\"\\n\\n\"],\n",
    "    \"tokens_to_generate\": 64,\n",
    "    \"temperature\": 0.5,\n",
    "    \"runtime_top_k\": 0,\n",
    "    \"runtime_top_p\": 1.0,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"beam_search_diversity_rate\": 0.0,\n",
    "    \"beam_width\": 1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"len_penalty\": 1.0,\n",
    "    \"is_return_log_probs\": IS_RETURN_LOG_PROBS,\n",
    "    \"start_id\": START_ID,\n",
    "    \"end_id\": END_ID,\n",
    "    \"bad_words_list\": [\"\"],\n",
    "}\n",
    "\n",
    "inputs = prepare_inputs(summarization)\n",
    "result = client.infer(MODEL, inputs)\n",
    "completions = prepare_outputs(result)\n",
    "for i in completions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25eec53",
   "metadata": {},
   "source": [
    "### AI Chatbot QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c327d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misty is a cheerful AI assistant and companion, created by NVIDIA engineers. Misty is clever and helpful, and will do everything it can to cheer you up:\n",
      "\n",
      "You: How are you feeling?\n",
      "Misty: I'm feeling great, how may I help you today?\n",
      "You: Can you please suggest a movie?\n",
      "Misty: How about \"The Martian\". It's a sci-fi movie about an astronaut getting stranded on Mars!\n",
      "You: That's cool! But i'm in the mood for watching comedy today\n",
      "Misty: How about \"The Hangover\"?\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "chatbot = {\n",
    "    \"prompt\": [[\"Misty is a cheerful AI assistant and companion, created by NVIDIA engineers. Misty is clever and helpful, and will do everything it can to cheer you up:\\n\\nYou: How are you feeling?\\nMisty: I'm feeling great, how may I help you today?\\nYou: Can you please suggest a movie?\\nMisty: How about \\\"The Martian\\\". It's a sci-fi movie about an astronaut getting stranded on Mars!\\nYou: That's cool! But i'm in the mood for watching comedy today\\nMisty:\"]],\n",
    "    \"stop_words_list\": [\"You:\"],\n",
    "    \"tokens_to_generate\": 40,\n",
    "    \"temperature\": 0.5,\n",
    "    \"runtime_top_k\": 2,\n",
    "    \"runtime_top_p\": 1.0,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"beam_search_diversity_rate\": 0.0,\n",
    "    \"beam_width\": 1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"len_penalty\": 1.0,\n",
    "    \"is_return_log_probs\": IS_RETURN_LOG_PROBS,\n",
    "    \"start_id\": START_ID,\n",
    "    \"end_id\": END_ID,\n",
    "    \"bad_words_list\": [\"\"],\n",
    "}\n",
    "\n",
    "inputs = prepare_inputs(chatbot)\n",
    "result = client.infer(MODEL, inputs)\n",
    "completions = prepare_outputs(result)\n",
    "for i in completions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769e5f3",
   "metadata": {},
   "source": [
    "### Write Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82443797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a python3 script that prints each item in a list.\n",
      "\n",
      "===\n",
      "\n",
      "    >>> my_list = [1, 2, 3]\n",
      "    >>> for item in my_list:\n",
      "    ...     print(item)\n",
      "    ...\n",
      "    1\n",
      "    2\n",
      "    3\n",
      "\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "write_code = {\n",
    "    \"prompt\": [[\"Write a python3 script that prints each item in a list.\\n\\n===\\n\\n\"]],\n",
    "    \"stop_words_list\": [\"===\",\"\\n\\n\"],\n",
    "    \"tokens_to_generate\": 128,\n",
    "    \"temperature\": 0.3,\n",
    "    \"runtime_top_k\": 32,\n",
    "    \"runtime_top_p\": 0.9,\n",
    "    \"random_seed\": 1,\n",
    "    \"beam_search_diversity_rate\": 0,\n",
    "    \"beam_width\": 1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"len_penalty\": 1.0,\n",
    "    \"is_return_log_probs\": IS_RETURN_LOG_PROBS,\n",
    "    \"start_id\": START_ID,\n",
    "    \"end_id\": END_ID,\n",
    "    \"bad_words_list\": [\"\"],\n",
    "}\n",
    "\n",
    "inputs = prepare_inputs(write_code)\n",
    "result = client.infer(MODEL, inputs)\n",
    "completions = prepare_outputs(result)\n",
    "for i in completions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdf7dc",
   "metadata": {},
   "source": [
    "### Unstructured Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ac0b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with 80 billion transistors using a cutting-edge TSMC 4N process custom tailored for NVIDIA’s accelerated compute needs, H100 is the world’s most advanced chip ever built. It features major advances to accelerate AI, HPC, memory bandwidth, interconnect, and communication at data center scale.\n",
      "\n",
      "The Hopper architecture’s second-generation MIG supports multi-tenant, multi-user configurations in virtualized environments, securely partitioning the GPU into isolated, right-size instances to maximize quality of service (QoS) for 7X more secured tenants.\n",
      "\n",
      "The Transformer Engine uses software and Hopper Tensor Core technology designed to accelerate training for models built from the world’s most important AI model building block, the transformer. Hopper Tensor Cores can apply mixed FP8 and FP16 precisions to dramatically accelerate AI calculations for transformers.\n",
      "\n",
      "The NVLink Switch System enables the scaling of multi-GPU input/output (IO) across multiple servers at 900 gigabytes per second (GB/s) bidirectional per GPU, over 7X the bandwidth of PCIe Gen5. The system supports clusters of up to 256 H100s and delivers 9X higher bandwidth than InfiniBand HDR on the NVIDIA Ampere architecture.\n",
      "\n",
      "NVIDIA Confidential Computing is a built-in security feature of Hopper that makes NVIDIA H100 the world’s first accelerator with confidential computing capabilities. Users can protect the confidentiality and integrity of their data and applications in use while accessing the unsurpassed acceleration of H100 GPUs.\n",
      "\n",
      "Hopper’s DPX instructions accelerate dynamic programming algorithms by 40X compared to CPUs and 7X compared to NVIDIA Ampere architecture GPUs. This leads to dramatically faster times in disease diagnosis, real-time routing optimizations, and graph analytics.\n",
      "\n",
      "Q: How many h100 GPUs can I connect with nvlink?\n",
      "A: You can connect up to 256 H100 GPUs with NVLink.\n",
      "\n",
      "Q:\n"
     ]
    }
   ],
   "source": [
    "unstructured_qa = {\n",
    "    \"prompt\": [[\"Built with 80 billion transistors using a cutting-edge TSMC 4N process custom tailored for NVIDIA’s accelerated compute needs, H100 is the world’s most advanced chip ever built. It features major advances to accelerate AI, HPC, memory bandwidth, interconnect, and communication at data center scale.\\n\\nThe Hopper architecture’s second-generation MIG supports multi-tenant, multi-user configurations in virtualized environments, securely partitioning the GPU into isolated, right-size instances to maximize quality of service (QoS) for 7X more secured tenants.\\n\\nThe Transformer Engine uses software and Hopper Tensor Core technology designed to accelerate training for models built from the world’s most important AI model building block, the transformer. Hopper Tensor Cores can apply mixed FP8 and FP16 precisions to dramatically accelerate AI calculations for transformers.\\n\\nThe NVLink Switch System enables the scaling of multi-GPU input/output (IO) across multiple servers at 900 gigabytes per second (GB/s) bidirectional per GPU, over 7X the bandwidth of PCIe Gen5. The system supports clusters of up to 256 H100s and delivers 9X higher bandwidth than InfiniBand HDR on the NVIDIA Ampere architecture.\\n\\nNVIDIA Confidential Computing is a built-in security feature of Hopper that makes NVIDIA H100 the world’s first accelerator with confidential computing capabilities. Users can protect the confidentiality and integrity of their data and applications in use while accessing the unsurpassed acceleration of H100 GPUs.\\n\\nHopper’s DPX instructions accelerate dynamic programming algorithms by 40X compared to CPUs and 7X compared to NVIDIA Ampere architecture GPUs. This leads to dramatically faster times in disease diagnosis, real-time routing optimizations, and graph analytics.\\n\\nQ: How many h100 GPUs can I connect with nvlink?\\nA:\"]],\n",
    "    \"stop_words_list\": [\"Q:\"],\n",
    "    \"tokens_to_generate\": 32,\n",
    "    \"temperature\": 0.2,\n",
    "    \"runtime_top_k\": 0,\n",
    "    \"runtime_top_p\": 1.0,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"beam_search_diversity_rate\": 0.0,\n",
    "    \"beam_width\": 1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"len_penalty\": 1.0,\n",
    "    \"is_return_log_probs\": IS_RETURN_LOG_PROBS,\n",
    "    \"start_id\": START_ID,\n",
    "    \"end_id\": END_ID,\n",
    "    \"bad_words_list\": [\"\"],\n",
    "}\n",
    "\n",
    "inputs = prepare_inputs(unstructured_qa)\n",
    "result = client.infer(MODEL, inputs)\n",
    "completions = prepare_outputs(result)\n",
    "for i in completions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc0b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
